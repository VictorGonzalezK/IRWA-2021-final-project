{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "VÃ­ctor GonzÃ¡lez Kullmann u161806<br>\n",
    "Albert Baito PanÃ© u161812\n",
    "\n",
    "## PART 1: Text Processing\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import nltk\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.stem import PorterStemmer\n",
    "from nltk.stem import SnowballStemmer\n",
    "import json\n",
    "import pycountry\n",
    "import re\n",
    "import string"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "#create a function to load the desired file containing the tweets\n",
    "def load_data(file_name):\n",
    "    with open(file_name) as f:\n",
    "        tweets=json.load(f)\n",
    "    return tweets\n",
    "        \n",
    "tweets = load_data(\"dataset_tweets_WHO.txt\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "#function that transforms the iso lenguage in the complete name\n",
    "def iso_leng_translate(leng):\n",
    "    return pycountry.languages.get(alpha_2=leng).name.lower()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'ja', 'ar', 'ru', 'fr', 'de', 'en', 'in', 'tl', 'uk', 'und', 'ps', 'es'}\n"
     ]
    }
   ],
   "source": [
    "#get all the languages in the tweets of the datasets and get the stopwords of that language if available\n",
    "def create_stopword_dict(tweets):\n",
    "    #get languages\n",
    "    lang=[]\n",
    "    for i in range(len(tweets)):\n",
    "        lang.append((tweets[str(i)]['lang']))\n",
    "    languages = set(lang)\n",
    "    print(languages)\n",
    "    \n",
    "    #search for the stopwords and save them in a dictionary for later usage\n",
    "    lang_dict = {}\n",
    "    for i in languages:     \n",
    "        try:\n",
    "            #transform ISO lenguage in complete name for convenience\n",
    "            lang_dict [i] = set(stopwords.words(iso_leng_translate(i)))\n",
    "            \n",
    "        #if we don't have a stopwords available for that lenguage we just skip\n",
    "        except:\n",
    "            continue\n",
    "        \n",
    "        \n",
    "    return lang_dict\n",
    "\n",
    "stopwords_bylang = create_stopword_dict(tweets)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def clean_data(tweets, stopwordsByLan):\n",
    "    \n",
    "    clean_texts = {}\n",
    "    \n",
    "    for i in range(len(tweets)):\n",
    "        #get the tweet text\n",
    "        og_tweet_text = (tweets[str(i)]['full_text']) \n",
    "        \n",
    "        #lowercase to uniform format and split to get words\n",
    "        og_tweet_text = og_tweet_text.lower()   \n",
    "        og_tweet_text = og_tweet_text.split()\n",
    "        \n",
    "        #create our pattern to avoid removing #\n",
    "        \n",
    "        #remove punctuation\n",
    "        tweet_text=[]\n",
    "        for word in og_tweet_text:\n",
    "            #maintain the links in the correct format\n",
    "            if \"https\" not in word:\n",
    "                #delete all punctuation conserving the # and @(in tweets are meaninguful)\n",
    "                word = re.sub(r'[^\\w\\s#@]','', word)\n",
    "                word = re.sub(r'_','',word)\n",
    "\n",
    "            if word:\n",
    "                tweet_text.append(word) \n",
    "        \n",
    "        \n",
    "        #get lenguage to filter stpowords\n",
    "        tweet_lang = tweets[str(i)]['lang'].lower()\n",
    "        #check availability of stopwors dict\n",
    "        if tweet_lang in stopwordsByLan.keys():\n",
    "            #if possible filter stopwords\n",
    "            stop_words = set(stopwordsByLan[tweet_lang])\n",
    "            clean_text = []\n",
    "            for word in tweet_text:\n",
    "                if word not in stop_words:\n",
    "                    clean_text.append(word)\n",
    "        \n",
    "        #if stopwords are not available in some lenguage just let them \n",
    "        else:\n",
    "            clean_text = tweet_text    \n",
    "\n",
    "        \n",
    "        #stem the words with the correct format for each lenguage\n",
    "        try:\n",
    "            stemmer = SnowballStemmer(iso_leng_translate(tweet_lang))\n",
    "            clean_text = [stemmer.stem(word) for word in clean_text]\n",
    "        \n",
    "        except:\n",
    "            pass\n",
    "        \n",
    "        clean_texts[tweets[str(i)]['id']] = clean_text\n",
    "        \n",
    "        \n",
    "    return clean_texts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['intern', 'day', 'disast', 'risk', 'reduct', '#openwho', 'launch', 'multiti', 'core', 'curriculum', 'help', 'equip', 'compet', 'need', 'work', 'within', 'public', 'health', 'emerg', 'respons', 'start', 'learn', 'today', 'amp', '#ready4respons', 'https://t.co/hbffof0xkl', 'https://t.co/fgzy22rwus']\n"
     ]
    }
   ],
   "source": [
    "cleanDataset = clean_data(tweets, stopwords_bylang)\n",
    "print(cleanDataset[1448215930178310144])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "#function created to search for tweet id of a concrete lenguage\n",
    "def tweet_lenguageSearcher(tweets, leng):\n",
    "    for i in range(len(tweets)):\n",
    "        if (tweets[str(i)]['lang']) == leng:\n",
    "            print(tweets[str(i)]['id'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1448163383493136385\n",
      "1447422682711068673\n",
      "1447421758437564417\n",
      "1447421622235942912\n",
      "1446492798493003779\n",
      "1437873802952708097\n",
      "1437873628792639490\n",
      "1436055322376949761\n",
      "1436055263312818176\n",
      "1426242406853353474\n",
      "1425836042599374848\n",
      "1425212438354571265\n",
      "1424986407857238034\n",
      "1423905220250243074\n",
      "1423662605516804097\n",
      "1422204917876461572\n",
      "1421895908669763586\n",
      "1411320867456434177\n",
      "1409244087962775557\n"
     ]
    }
   ],
   "source": [
    "tweet_lenguageSearcher(tweets, \"es\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "#function to plot the og tweet to check for the data processing correctness\n",
    "def tweet_Searcher(tweets, id):\n",
    "    for i in range(len(tweets)):\n",
    "            if (tweets[str(i)]['id']) == id:\n",
    "                print(tweets[str(i)]['full_text'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "It's International Day for Disaster Risk Reduction\n",
      "\n",
      "#OpenWHO has launched a multi-tiered core curriculum to help equip you with the competencies needed to work within public health emergency response.\n",
      "\n",
      "Start learning today &amp; be #Ready4Response:\n",
      "ðŸ‘‰ https://t.co/hBFFOF0xKL https://t.co/fgZY22RWuS\n"
     ]
    }
   ],
   "source": [
    "tweet_Searcher(tweets, 1448215930178310144)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
